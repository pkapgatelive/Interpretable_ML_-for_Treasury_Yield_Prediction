{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 02. Data Cleaning & Feature Engineering\n",
        "\n",
        "## Purpose\n",
        "This notebook implements Phase 2 of the yield curve forecasting project: transforming raw daily yield curve and macroeconomic data into a clean, consistent, and well-engineered dataset suitable for supervised learning.\n",
        "\n",
        "## Objectives\n",
        "1. **Data Frequency Alignment** - Align all datasets to common daily frequency\n",
        "2. **Gap Handling** - Forward-fill missing values and remove non-trading days  \n",
        "3. **Yield Transformation** - Transform yields to continuously compounded rates if needed\n",
        "4. **Yield Curve Features** - Generate slope, curvature, and PCA-based features\n",
        "5. **Macro Feature Engineering** - Create lagged macro indicators without look-ahead bias\n",
        "6. **Standardization** - Apply consistent z-score normalization\n",
        "7. **Model-Ready Data** - Construct final feature matrix (X) and target matrix (Y)\n",
        "\n",
        "## Expected Outputs\n",
        "- Clean feature matrix (X) with engineered features\n",
        "- Target matrix (Y) with future yield values\n",
        "- Processed data saved to `/data/processed/`\n",
        "- Documentation of all transformations applied\n",
        "\n",
        "## Dependencies\n",
        "- pandas, numpy, sklearn for data manipulation\n",
        "- Raw yield curve and macro data (generated if not available)\n",
        "- Configuration parameters from config files\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Create necessary directories\n",
        "Path(\"../data/processed\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"../data/features\").mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Data Loading and Generation\n",
        "\n",
        "Since the raw data sources require API keys and proper configuration, we'll generate realistic sample data that represents:\n",
        "- **Treasury Yield Curves**: Daily yields for tenors 1M, 3M, 6M, 1Y, 2Y, 3Y, 5Y, 7Y, 10Y, 20Y, 30Y\n",
        "- **Macroeconomic Indicators**: Fed Funds Rate, CPI, PMI, Unemployment Rate, VIX, and other key indicators\n",
        "\n",
        "This sample data will demonstrate the complete feature engineering pipeline that can be applied to real data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_realistic_yield_data(start_date='2000-01-01', end_date='2024-12-01'):\n",
        "    \"\"\"\n",
        "    Generate realistic Treasury yield curve data with proper term structure characteristics.\n",
        "    \"\"\"\n",
        "    # Create business day range\n",
        "    date_range = pd.bdate_range(start=start_date, end=end_date, freq='B')\n",
        "    n_days = len(date_range)\n",
        "    \n",
        "    # Define tenors in years\n",
        "    tenors = [1/12, 3/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30]\n",
        "    tenor_names = ['1M', '3M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\n",
        "    \n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate base yield curve with realistic term structure\n",
        "    # Base rates follow historical patterns with upward sloping curve\n",
        "    base_rates = np.array([0.5, 0.8, 1.2, 1.8, 2.5, 2.8, 3.2, 3.5, 3.8, 4.2, 4.5])\n",
        "    \n",
        "    # Generate time-varying factors (level, slope, curvature)\n",
        "    time_factor = np.linspace(0, 1, n_days)\n",
        "    \n",
        "    # Level factor (overall interest rate environment)\n",
        "    level_factor = 2.0 + 1.5 * np.sin(2 * np.pi * time_factor * 3) + \\\n",
        "                   np.cumsum(np.random.normal(0, 0.01, n_days))\n",
        "    \n",
        "    # Slope factor (yield curve steepness)\n",
        "    slope_factor = 0.3 + 0.2 * np.sin(2 * np.pi * time_factor * 2) + \\\n",
        "                   np.cumsum(np.random.normal(0, 0.005, n_days))\n",
        "    \n",
        "    # Curvature factor (mid-curve behavior)\n",
        "    curvature_factor = 0.1 * np.sin(2 * np.pi * time_factor * 4) + \\\n",
        "                       np.cumsum(np.random.normal(0, 0.003, n_days))\n",
        "    \n",
        "    # Generate yields for each tenor\n",
        "    yields_data = {}\n",
        "    \n",
        "    for i, (tenor, name) in enumerate(zip(tenors, tenor_names)):\n",
        "        # Base yield with term structure\n",
        "        base_yield = base_rates[i]\n",
        "        \n",
        "        # Apply factors with tenor-specific loadings\n",
        "        level_loading = 1.0\n",
        "        slope_loading = tenor  # Longer tenors more sensitive to slope\n",
        "        curvature_loading = tenor * (30 - tenor) / 100  # Hump-shaped loading\n",
        "        \n",
        "        yields = (base_yield + \n",
        "                 level_loading * level_factor +\n",
        "                 slope_loading * slope_factor +\n",
        "                 curvature_loading * curvature_factor +\n",
        "                 np.random.normal(0, 0.05, n_days))  # Idiosyncratic noise\n",
        "        \n",
        "        # Ensure yields are positive\n",
        "        yields = np.maximum(yields, 0.01)\n",
        "        \n",
        "        yields_data[name] = yields\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df_yields = pd.DataFrame(yields_data, index=date_range)\n",
        "    df_yields.index.name = 'date'\n",
        "    df_yields = df_yields.reset_index()\n",
        "    \n",
        "    logger.info(f\"Generated {len(df_yields)} days of yield curve data\")\n",
        "    return df_yields\n",
        "\n",
        "def generate_realistic_macro_data(start_date='2000-01-01', end_date='2024-12-01'):\n",
        "    \"\"\"\n",
        "    Generate realistic macroeconomic indicator data.\n",
        "    \"\"\"\n",
        "    # Create business day range for daily indicators and monthly for others\n",
        "    daily_range = pd.bdate_range(start=start_date, end=end_date, freq='B')\n",
        "    monthly_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "    \n",
        "    n_daily = len(daily_range)\n",
        "    n_monthly = len(monthly_range)\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate daily indicators\n",
        "    daily_data = {}\n",
        "    \n",
        "    # Federal Funds Rate (daily)\n",
        "    fed_funds = 2.0 + 3.0 * np.sin(np.linspace(0, 4*np.pi, n_daily)) + \\\n",
        "                np.cumsum(np.random.normal(0, 0.02, n_daily))\n",
        "    fed_funds = np.maximum(fed_funds, 0.0)  # Non-negative\n",
        "    daily_data['fed_funds_rate'] = fed_funds\n",
        "    \n",
        "    # VIX (volatility index)\n",
        "    vix = 20 + 10 * np.sin(np.linspace(0, 6*np.pi, n_daily)) + \\\n",
        "          np.cumsum(np.random.normal(0, 0.5, n_daily))\n",
        "    vix = np.maximum(vix, 5.0)  # Minimum VIX\n",
        "    daily_data['vix'] = vix\n",
        "    \n",
        "    # TED Spread\n",
        "    ted_spread = 0.3 + 0.5 * np.sin(np.linspace(0, 5*np.pi, n_daily)) + \\\n",
        "                 np.cumsum(np.random.normal(0, 0.01, n_daily))\n",
        "    ted_spread = np.maximum(ted_spread, 0.0)\n",
        "    daily_data['ted_spread'] = ted_spread\n",
        "    \n",
        "    # 5Y and 10Y Breakeven Inflation\n",
        "    infl_5y = 2.0 + 0.5 * np.sin(np.linspace(0, 3*np.pi, n_daily)) + \\\n",
        "              np.cumsum(np.random.normal(0, 0.01, n_daily))\n",
        "    infl_10y = 2.2 + 0.4 * np.sin(np.linspace(0, 3*np.pi, n_daily)) + \\\n",
        "               np.cumsum(np.random.normal(0, 0.008, n_daily))\n",
        "    daily_data['breakeven_5y'] = infl_5y\n",
        "    daily_data['breakeven_10y'] = infl_10y\n",
        "    \n",
        "    # Create daily DataFrame\n",
        "    df_daily = pd.DataFrame(daily_data, index=daily_range)\n",
        "    \n",
        "    # Generate monthly indicators and interpolate to daily\n",
        "    monthly_data = {}\n",
        "    \n",
        "    # CPI (monthly, interpolated to daily)\n",
        "    cpi_base = 250\n",
        "    cpi_growth = np.cumsum(np.random.normal(0.002, 0.001, n_monthly))  # ~2.4% annual\n",
        "    cpi = cpi_base * np.exp(cpi_growth)\n",
        "    monthly_data['cpi'] = cpi\n",
        "    \n",
        "    # Core CPI\n",
        "    core_cpi = cpi * (1 + np.random.normal(0, 0.001, n_monthly))\n",
        "    monthly_data['core_cpi'] = core_cpi\n",
        "    \n",
        "    # Unemployment Rate\n",
        "    unemployment = 5.0 + 2.0 * np.sin(np.linspace(0, 2*np.pi, n_monthly)) + \\\n",
        "                   np.cumsum(np.random.normal(0, 0.05, n_monthly))\n",
        "    unemployment = np.clip(unemployment, 2.0, 15.0)\n",
        "    monthly_data['unemployment_rate'] = unemployment\n",
        "    \n",
        "    # ISM Manufacturing PMI\n",
        "    pmi = 52 + 5 * np.sin(np.linspace(0, 3*np.pi, n_monthly)) + \\\n",
        "          np.random.normal(0, 2, n_monthly)\n",
        "    pmi = np.clip(pmi, 30, 70)\n",
        "    monthly_data['ism_pmi'] = pmi\n",
        "    \n",
        "    # Industrial Production Index\n",
        "    ip_growth = np.cumsum(np.random.normal(0.0015, 0.003, n_monthly))  # ~1.8% annual\n",
        "    ip = 100 * np.exp(ip_growth)\n",
        "    monthly_data['industrial_production'] = ip\n",
        "    \n",
        "    # Nonfarm Payrolls (monthly change in thousands)\n",
        "    payrolls = np.random.normal(150, 50, n_monthly)  # Average 150k jobs per month\n",
        "    monthly_data['payrolls_change'] = payrolls\n",
        "    \n",
        "    # Consumer Sentiment\n",
        "    sentiment = 85 + 15 * np.sin(np.linspace(0, 2*np.pi, n_monthly)) + \\\n",
        "                np.random.normal(0, 5, n_monthly)\n",
        "    sentiment = np.clip(sentiment, 50, 120)\n",
        "    monthly_data['consumer_sentiment'] = sentiment\n",
        "    \n",
        "    # Create monthly DataFrame and interpolate to daily\n",
        "    df_monthly = pd.DataFrame(monthly_data, index=monthly_range)\n",
        "    \n",
        "    # Reindex to daily and forward fill\n",
        "    df_monthly_daily = df_monthly.reindex(daily_range, method='ffill')\n",
        "    \n",
        "    # Combine daily and monthly data\n",
        "    df_macro = pd.concat([df_daily, df_monthly_daily], axis=1)\n",
        "    df_macro.index.name = 'date'\n",
        "    df_macro = df_macro.reset_index()\n",
        "    \n",
        "    logger.info(f\"Generated {len(df_macro)} days of macroeconomic data\")\n",
        "    return df_macro\n",
        "\n",
        "# Generate the data\n",
        "print(\"üîÑ Generating realistic yield curve data...\")\n",
        "df_yields = generate_realistic_yield_data()\n",
        "\n",
        "print(\"üîÑ Generating realistic macroeconomic data...\")\n",
        "df_macro = generate_realistic_macro_data()\n",
        "\n",
        "print(\"‚úÖ Data generation completed successfully!\")\n",
        "print(f\"Yield data shape: {df_yields.shape}\")\n",
        "print(f\"Macro data shape: {df_macro.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 2. Data Frequency Alignment & Gap Handling\n",
        "\n",
        "This section aligns all datasets to a common daily frequency and handles missing values appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Align datasets on common date index\n",
        "print(\"üîÑ Aligning yield curve and macro datasets...\")\n",
        "\n",
        "# Merge yield and macro data on date\n",
        "df_combined = pd.merge(df_yields, df_macro, on='date', how='inner')\n",
        "\n",
        "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
        "print(f\"Date range: {df_combined['date'].min()} to {df_combined['date'].max()}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df_combined.isnull().sum()\n",
        "print(\"\\nüìä Missing Values by Column:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Forward fill missing values (common for financial time series)\n",
        "print(\"\\nüîÑ Forward-filling missing values...\")\n",
        "df_combined = df_combined.fillna(method='ffill')\n",
        "\n",
        "# Check for any remaining missing values\n",
        "remaining_missing = df_combined.isnull().sum().sum()\n",
        "print(f\"Remaining missing values: {remaining_missing}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nüìà Basic Statistics:\")\n",
        "print(df_combined.describe().round(3))\n",
        "\n",
        "# Display first few rows to verify data structure\n",
        "print(\"\\nüìã First 5 rows of combined dataset:\")\n",
        "print(df_combined.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 3. Yield Transformation\n",
        "\n",
        "Convert nominal yield values to continuously compounded rates for consistency in growth-based forecasting applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define yield columns for transformation\n",
        "yield_columns = ['1M', '3M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\n",
        "\n",
        "# Store original yields for comparison\n",
        "df_original_yields = df_combined[yield_columns].copy()\n",
        "\n",
        "print(\"üîÑ Converting yields to continuously compounded rates...\")\n",
        "\n",
        "# Convert from annual percentage to continuously compounded rates\n",
        "# Formula: r_cc = ln(1 + r_annual/100)\n",
        "for col in yield_columns:\n",
        "    df_combined[f'{col}_cc'] = np.log(1 + df_combined[col] / 100)\n",
        "\n",
        "# Create separate dataframe with continuously compounded yields\n",
        "yield_cc_columns = [f'{col}_cc' for col in yield_columns]\n",
        "df_yields_cc = df_combined[['date'] + yield_cc_columns].copy()\n",
        "\n",
        "print(\"‚úÖ Yield transformation completed\")\n",
        "\n",
        "# Visualize the transformation effect\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot original vs transformed yields for a few key tenors\n",
        "tenors_to_plot = ['2Y', '10Y']\n",
        "for i, tenor in enumerate(tenors_to_plot):\n",
        "    # Original yields\n",
        "    axes[0, i].plot(df_combined['date'], df_combined[tenor], label='Original (%)', alpha=0.7)\n",
        "    axes[0, i].set_title(f'{tenor} Treasury Yield - Original')\n",
        "    axes[0, i].set_ylabel('Yield (%)')\n",
        "    axes[0, i].grid(True, alpha=0.3)\n",
        "    axes[0, i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Transformed yields\n",
        "    axes[1, i].plot(df_combined['date'], df_combined[f'{tenor}_cc'], \n",
        "                   label='Continuously Compounded', color='red', alpha=0.7)\n",
        "    axes[1, i].set_title(f'{tenor} Treasury Yield - Continuously Compounded')\n",
        "    axes[1, i].set_ylabel('Continuously Compounded Rate')\n",
        "    axes[1, i].set_xlabel('Date')\n",
        "    axes[1, i].grid(True, alpha=0.3)\n",
        "    axes[1, i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display transformation statistics\n",
        "print(\"\\nüìä Transformation Statistics:\")\n",
        "print(\"Original Yields (first 5 tenors):\")\n",
        "print(df_original_yields[yield_columns[:5]].describe().round(4))\n",
        "print(\"\\nContinuously Compounded Yields (first 5 tenors):\")\n",
        "print(df_combined[yield_cc_columns[:5]].describe().round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 4. Derived Yield Curve Features\n",
        "\n",
        "Generate structural indicators of yield curve shape that capture key market dynamics:\n",
        "- **Slope**: 10Y - 2Y spread (most common measure of yield curve steepness)\n",
        "- **Curvature**: (2Y + 30Y) - 2√ó(10Y) (captures mid-curve flattening/steepening)  \n",
        "- **PCA Scores**: Principal components capturing level, slope, and curvature factors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Creating derived yield curve features...\")\n",
        "\n",
        "# 1. SLOPE: 10Y - 2Y spread\n",
        "df_combined['yield_slope_10y2y'] = df_combined['10Y'] - df_combined['2Y']\n",
        "df_combined['yield_slope_10y2y_cc'] = df_combined['10Y_cc'] - df_combined['2Y_cc']\n",
        "\n",
        "# Additional slope measures\n",
        "df_combined['yield_slope_30y2y'] = df_combined['30Y'] - df_combined['2Y']\n",
        "df_combined['yield_slope_10y3m'] = df_combined['10Y'] - df_combined['3M']\n",
        "\n",
        "print(\"‚úÖ Slope features created\")\n",
        "\n",
        "# 2. CURVATURE: (2Y + 30Y) - 2*(10Y)\n",
        "df_combined['yield_curvature'] = (df_combined['2Y'] + df_combined['30Y']) - 2 * df_combined['10Y']\n",
        "df_combined['yield_curvature_cc'] = (df_combined['2Y_cc'] + df_combined['30Y_cc']) - 2 * df_combined['10Y_cc']\n",
        "\n",
        "# Alternative curvature measure: butterfly spread\n",
        "df_combined['yield_butterfly_5y'] = (df_combined['3Y'] + df_combined['7Y']) - 2 * df_combined['5Y']\n",
        "\n",
        "print(\"‚úÖ Curvature features created\")\n",
        "\n",
        "# 3. PCA ANALYSIS: Extract principal components from yield curve\n",
        "print(\"üîÑ Performing PCA analysis on yield curve...\")\n",
        "\n",
        "# Prepare data for PCA (using continuously compounded yields)\n",
        "pca_data = df_combined[yield_cc_columns].copy()\n",
        "\n",
        "# Standardize data before PCA\n",
        "scaler_pca = StandardScaler()\n",
        "pca_data_scaled = scaler_pca.fit_transform(pca_data)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=5)  # First 5 components capture most variance\n",
        "pca_scores = pca.fit_transform(pca_data_scaled)\n",
        "\n",
        "# Add PCA scores to dataframe\n",
        "for i in range(5):\n",
        "    df_combined[f'pca_factor_{i+1}'] = pca_scores[:, i]\n",
        "\n",
        "print(\"‚úÖ PCA analysis completed\")\n",
        "\n",
        "# Display PCA results\n",
        "print(f\"\\nüìä PCA Results:\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.round(4)}\")\n",
        "print(f\"Cumulative variance explained: {np.cumsum(pca.explained_variance_ratio_).round(4)}\")\n",
        "\n",
        "# Analyze PCA loadings\n",
        "pca_loadings = pd.DataFrame(\n",
        "    pca.components_[:3].T,  # First 3 components\n",
        "    columns=['PC1 (Level)', 'PC2 (Slope)', 'PC3 (Curvature)'],\n",
        "    index=yield_columns\n",
        ")\n",
        "\n",
        "print(\"\\nüìã PCA Loadings (First 3 Components):\")\n",
        "print(pca_loadings.round(4))\n",
        "\n",
        "# Visualize PCA loadings\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (col, title) in enumerate([('PC1 (Level)', 'Level Factor'), \n",
        "                                 ('PC2 (Slope)', 'Slope Factor'), \n",
        "                                 ('PC3 (Curvature)', 'Curvature Factor')]):\n",
        "    axes[i].bar(pca_loadings.index, pca_loadings[col], alpha=0.7)\n",
        "    axes[i].set_title(f'{title} - PCA Loadings')\n",
        "    axes[i].set_xlabel('Tenor')\n",
        "    axes[i].set_ylabel('Loading')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. ADDITIONAL YIELD CURVE FEATURES\n",
        "\n",
        "# Term structure level (average of all yields)\n",
        "df_combined['yield_level'] = df_combined[yield_columns].mean(axis=1)\n",
        "\n",
        "# Short-end vs long-end average\n",
        "short_tenors = ['1M', '3M', '6M', '1Y']\n",
        "long_tenors = ['10Y', '20Y', '30Y']\n",
        "df_combined['yield_short_avg'] = df_combined[short_tenors].mean(axis=1)\n",
        "df_combined['yield_long_avg'] = df_combined[long_tenors].mean(axis=1)\n",
        "df_combined['yield_short_long_spread'] = df_combined['yield_long_avg'] - df_combined['yield_short_avg']\n",
        "\n",
        "# Yield curve range (max - min)\n",
        "df_combined['yield_range'] = df_combined[yield_columns].max(axis=1) - df_combined[yield_columns].min(axis=1)\n",
        "\n",
        "print(\"‚úÖ Additional yield curve features created\")\n",
        "\n",
        "# Create summary of yield curve features\n",
        "yield_curve_features = [\n",
        "    'yield_slope_10y2y', 'yield_slope_10y2y_cc', 'yield_slope_30y2y', 'yield_slope_10y3m',\n",
        "    'yield_curvature', 'yield_curvature_cc', 'yield_butterfly_5y',\n",
        "    'pca_factor_1', 'pca_factor_2', 'pca_factor_3', 'pca_factor_4', 'pca_factor_5',\n",
        "    'yield_level', 'yield_short_avg', 'yield_long_avg', 'yield_short_long_spread', 'yield_range'\n",
        "]\n",
        "\n",
        "print(f\"\\nüìà Created {len(yield_curve_features)} yield curve features:\")\n",
        "for feature in yield_curve_features:\n",
        "    print(f\"  ‚Ä¢ {feature}\")\n",
        "\n",
        "# Display statistics for key features\n",
        "print(\"\\nüìä Key Yield Curve Features Statistics:\")\n",
        "key_features = ['yield_slope_10y2y', 'yield_curvature', 'pca_factor_1', 'pca_factor_2', 'pca_factor_3']\n",
        "print(df_combined[key_features].describe().round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5. Macroeconomic Feature Engineering\n",
        "\n",
        "Create lagged versions of macroeconomic indicators to avoid look-ahead bias. This ensures we only use past information available at each prediction point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Creating macroeconomic features with proper lags...\")\n",
        "\n",
        "# Define macro variables and their characteristics\n",
        "macro_variables = {\n",
        "    # Policy and rates\n",
        "    'fed_funds_rate': {'lags': [1, 5, 22], 'transform': ['level', 'change']},\n",
        "    'ted_spread': {'lags': [1, 5, 22], 'transform': ['level', 'change']},\n",
        "    \n",
        "    # Inflation expectations  \n",
        "    'breakeven_5y': {'lags': [1, 5, 22], 'transform': ['level', 'change']},\n",
        "    'breakeven_10y': {'lags': [1, 5, 22], 'transform': ['level', 'change']},\n",
        "    \n",
        "    # Financial markets\n",
        "    'vix': {'lags': [1, 5, 22], 'transform': ['level', 'change', 'log']},\n",
        "    \n",
        "    # Economic indicators (monthly, already forward-filled)\n",
        "    'cpi': {'lags': [1, 22, 66], 'transform': ['pct_change']},  # 1 day, 1 month, 3 months\n",
        "    'core_cpi': {'lags': [1, 22, 66], 'transform': ['pct_change']},\n",
        "    'unemployment_rate': {'lags': [1, 22, 66], 'transform': ['level', 'change']},\n",
        "    'ism_pmi': {'lags': [1, 22, 66], 'transform': ['level', 'change']},\n",
        "    'industrial_production': {'lags': [1, 22, 66], 'transform': ['pct_change']},\n",
        "    'consumer_sentiment': {'lags': [1, 22, 66], 'transform': ['level', 'change']},\n",
        "}\n",
        "\n",
        "# Function to create lagged features\n",
        "def create_lagged_features(df, variable, lags, transforms):\n",
        "    \"\"\"Create lagged and transformed features for a variable.\"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    for lag in lags:\n",
        "        # Create base lagged variable\n",
        "        lagged_var = df[variable].shift(lag)\n",
        "        \n",
        "        for transform in transforms:\n",
        "            if transform == 'level':\n",
        "                features[f'{variable}_lag{lag}'] = lagged_var\n",
        "                \n",
        "            elif transform == 'change':\n",
        "                # First difference\n",
        "                features[f'{variable}_change_lag{lag}'] = lagged_var.diff()\n",
        "                \n",
        "            elif transform == 'pct_change':\n",
        "                # Percentage change\n",
        "                features[f'{variable}_pctchg_lag{lag}'] = lagged_var.pct_change()\n",
        "                \n",
        "            elif transform == 'log':\n",
        "                # Log transformation (for variables like VIX)\n",
        "                features[f'{variable}_log_lag{lag}'] = np.log(lagged_var + 1e-8)  # Small constant to avoid log(0)\n",
        "                \n",
        "    return features\n",
        "\n",
        "# Create all macro features\n",
        "macro_features = {}\n",
        "\n",
        "for variable, config in macro_variables.items():\n",
        "    if variable in df_combined.columns:\n",
        "        var_features = create_lagged_features(\n",
        "            df_combined, \n",
        "            variable, \n",
        "            config['lags'], \n",
        "            config['transform']\n",
        "        )\n",
        "        macro_features.update(var_features)\n",
        "        print(f\"‚úÖ Created {len(var_features)} features for {variable}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Variable {variable} not found in dataset\")\n",
        "\n",
        "# Add macro features to main dataframe\n",
        "for feature_name, feature_data in macro_features.items():\n",
        "    df_combined[feature_name] = feature_data\n",
        "\n",
        "print(f\"\\nüìà Total macro features created: {len(macro_features)}\")\n",
        "\n",
        "# Create additional derived macro features\n",
        "print(\"\\nüîÑ Creating derived macro features...\")\n",
        "\n",
        "# Interest rate momentum\n",
        "if 'fed_funds_rate_lag1' in df_combined.columns:\n",
        "    df_combined['fed_funds_momentum_5d'] = (\n",
        "        df_combined['fed_funds_rate_lag1'] - df_combined['fed_funds_rate_lag5']\n",
        "    )\n",
        "    df_combined['fed_funds_momentum_22d'] = (\n",
        "        df_combined['fed_funds_rate_lag1'] - df_combined['fed_funds_rate_lag22']\n",
        "    )\n",
        "\n",
        "# Inflation expectations spread\n",
        "if all(col in df_combined.columns for col in ['breakeven_10y_lag1', 'breakeven_5y_lag1']):\n",
        "    df_combined['inflation_term_spread'] = (\n",
        "        df_combined['breakeven_10y_lag1'] - df_combined['breakeven_5y_lag1']\n",
        "    )\n",
        "\n",
        "# Risk sentiment indicators\n",
        "if 'vix_log_lag1' in df_combined.columns:\n",
        "    df_combined['vix_percentile_22d'] = (\n",
        "        df_combined['vix_log_lag1'].rolling(22).rank() / 22\n",
        "    )\n",
        "\n",
        "# Economic activity momentum\n",
        "if all(col in df_combined.columns for col in ['ism_pmi_lag1', 'ism_pmi_lag22']):\n",
        "    df_combined['economic_momentum'] = (\n",
        "        0.5 * (df_combined['ism_pmi_lag1'] - 50) +  # PMI relative to neutral\n",
        "        0.3 * df_combined['ism_pmi_change_lag1'] +   # Recent PMI change\n",
        "        0.2 * (df_combined['unemployment_rate_lag22'] - df_combined['unemployment_rate_lag1'])  # Employment improvement\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Derived macro features created\")\n",
        "\n",
        "# Remove rows with NaN values created by lagging (keep sufficient history)\n",
        "initial_rows = len(df_combined)\n",
        "df_combined = df_combined.dropna()\n",
        "final_rows = len(df_combined)\n",
        "\n",
        "print(f\"\\nüìä Data summary after feature engineering:\")\n",
        "print(f\"Rows before NaN removal: {initial_rows}\")\n",
        "print(f\"Rows after NaN removal: {final_rows}\")\n",
        "print(f\"Rows removed: {initial_rows - final_rows}\")\n",
        "\n",
        "# Get list of all macro feature columns\n",
        "all_macro_features = [col for col in df_combined.columns if any(\n",
        "    col.startswith(var) for var in macro_variables.keys()\n",
        ") and col not in macro_variables.keys()]\n",
        "\n",
        "# Add derived features\n",
        "derived_features = [\n",
        "    'fed_funds_momentum_5d', 'fed_funds_momentum_22d', \n",
        "    'inflation_term_spread', 'vix_percentile_22d', 'economic_momentum'\n",
        "]\n",
        "all_macro_features.extend([f for f in derived_features if f in df_combined.columns])\n",
        "\n",
        "print(f\"\\nüìà Total engineered macro features: {len(all_macro_features)}\")\n",
        "\n",
        "# Display statistics for key macro features\n",
        "key_macro_features = [f for f in all_macro_features[:10] if f in df_combined.columns]\n",
        "if key_macro_features:\n",
        "    print(f\"\\nüìä Key Macro Features Statistics (showing first 10):\")\n",
        "    print(df_combined[key_macro_features].describe().round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 6. Standardization and Normalization\n",
        "\n",
        "Apply z-score standardization to all continuous variables while maintaining train/test consistency. This ensures that scaling statistics are computed only from the training window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Preparing data for standardization...\")\n",
        "\n",
        "# Define feature groups for standardization\n",
        "feature_groups = {\n",
        "    'yield_levels': yield_columns,  # Original yield levels\n",
        "    'yield_continuous': yield_cc_columns,  # Continuously compounded yields\n",
        "    'yield_curve_features': yield_curve_features,  # Derived yield curve features\n",
        "    'macro_features': all_macro_features,  # All macro features\n",
        "}\n",
        "\n",
        "# Combine all features to be standardized\n",
        "all_features = []\n",
        "for group_features in feature_groups.values():\n",
        "    all_features.extend([f for f in group_features if f in df_combined.columns])\n",
        "\n",
        "# Remove duplicates while preserving order\n",
        "all_features = list(dict.fromkeys(all_features))\n",
        "\n",
        "print(f\"üìä Total features to standardize: {len(all_features)}\")\n",
        "print(f\"Feature breakdown:\")\n",
        "for group_name, group_features in feature_groups.items():\n",
        "    available_features = [f for f in group_features if f in df_combined.columns]\n",
        "    print(f\"  ‚Ä¢ {group_name}: {len(available_features)} features\")\n",
        "\n",
        "# Create train/test split (80/20) while preserving time series order\n",
        "# This is important for avoiding look-ahead bias in standardization\n",
        "split_date = df_combined['date'].quantile(0.8)\n",
        "train_mask = df_combined['date'] <= split_date\n",
        "test_mask = df_combined['date'] > split_date\n",
        "\n",
        "print(f\"\\nüìÖ Data split:\")\n",
        "print(f\"Train period: {df_combined.loc[train_mask, 'date'].min()} to {df_combined.loc[train_mask, 'date'].max()}\")\n",
        "print(f\"Test period: {df_combined.loc[test_mask, 'date'].min()} to {df_combined.loc[test_mask, 'date'].max()}\")\n",
        "print(f\"Train samples: {train_mask.sum()}\")\n",
        "print(f\"Test samples: {test_mask.sum()}\")\n",
        "\n",
        "# Initialize scalers for different feature groups\n",
        "scalers = {}\n",
        "df_scaled = df_combined.copy()\n",
        "\n",
        "print(f\"\\nüîÑ Applying z-score standardization by feature group...\")\n",
        "\n",
        "for group_name, group_features in feature_groups.items():\n",
        "    available_features = [f for f in group_features if f in df_combined.columns]\n",
        "    \n",
        "    if not available_features:\n",
        "        continue\n",
        "        \n",
        "    print(f\"Processing {group_name} ({len(available_features)} features)...\")\n",
        "    \n",
        "    # Initialize scaler for this group\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit scaler only on training data\n",
        "    train_data = df_combined.loc[train_mask, available_features]\n",
        "    scaler.fit(train_data)\n",
        "    \n",
        "    # Transform both train and test data\n",
        "    df_scaled.loc[:, available_features] = scaler.transform(df_combined[available_features])\n",
        "    \n",
        "    # Store scaler for later use\n",
        "    scalers[group_name] = scaler\n",
        "    \n",
        "    # Display scaling statistics\n",
        "    print(f\"  ‚úÖ {group_name} standardized\")\n",
        "    print(f\"     Mean (train): {train_data.mean().mean():.4f}\")\n",
        "    print(f\"     Std (train): {train_data.std().mean():.4f}\")\n",
        "    print(f\"     Mean (scaled): {df_scaled.loc[train_mask, available_features].mean().mean():.4f}\")\n",
        "    print(f\"     Std (scaled): {df_scaled.loc[train_mask, available_features].std().mean():.4f}\")\n",
        "\n",
        "print(\"‚úÖ Standardization completed\")\n",
        "\n",
        "# Verify standardization worked correctly\n",
        "print(f\"\\nüîç Standardization verification:\")\n",
        "sample_features = all_features[:5]  # Check first 5 features\n",
        "train_scaled_stats = df_scaled.loc[train_mask, sample_features].describe()\n",
        "print(\"Train set statistics for sample features (should have mean‚âà0, std‚âà1):\")\n",
        "print(train_scaled_stats.loc[['mean', 'std']].round(4))\n",
        "\n",
        "# Create metadata for reproducibility\n",
        "scaling_metadata = {\n",
        "    'scaling_date': datetime.now().isoformat(),\n",
        "    'train_split_date': split_date.isoformat() if hasattr(split_date, 'isoformat') else str(split_date),\n",
        "    'train_samples': int(train_mask.sum()),\n",
        "    'test_samples': int(test_mask.sum()),\n",
        "    'feature_groups': {\n",
        "        group: len([f for f in features if f in df_combined.columns])\n",
        "        for group, features in feature_groups.items()\n",
        "    },\n",
        "    'total_features_scaled': len(all_features)\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã Scaling metadata:\")\n",
        "for key, value in scaling_metadata.items():\n",
        "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "# Save scaling metadata to be included with processed data\n",
        "import json\n",
        "with open('../data/processed/scaling_metadata.json', 'w') as f:\n",
        "    json.dump(scaling_metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Scaling metadata saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 7. Model-Ready Data Construction\n",
        "\n",
        "Construct the final feature matrix (X) and target matrix (Y) for supervised learning. This includes creating forward-looking targets while ensuring no look-ahead bias in features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Constructing model-ready feature matrix (X) and target matrix (Y)...\")\n",
        "\n",
        "# FEATURE MATRIX (X) CONSTRUCTION\n",
        "# ================================\n",
        "\n",
        "# Select features for modeling (excluding raw macro variables to avoid redundancy)\n",
        "feature_columns = []\n",
        "\n",
        "# Add yield curve features\n",
        "feature_columns.extend(yield_curve_features)\n",
        "\n",
        "# Add engineered macro features (excluding original macro variables)\n",
        "feature_columns.extend(all_macro_features)\n",
        "\n",
        "# Ensure all feature columns exist in the scaled dataframe\n",
        "feature_columns = [col for col in feature_columns if col in df_scaled.columns]\n",
        "\n",
        "print(f\"üìä Feature matrix dimensions:\")\n",
        "print(f\"  ‚Ä¢ Total features selected: {len(feature_columns)}\")\n",
        "\n",
        "# Create feature matrix X\n",
        "X = df_scaled[['date'] + feature_columns].copy()\n",
        "\n",
        "print(f\"  ‚Ä¢ Feature matrix shape: {X.shape}\")\n",
        "\n",
        "# TARGET MATRIX (Y) CONSTRUCTION\n",
        "# ===============================\n",
        "\n",
        "# Define target variables (future yield values)\n",
        "# We'll predict 1-day, 5-day, and 22-day ahead yields for key tenors\n",
        "target_tenors = ['2Y', '5Y', '10Y', '30Y']  # Key tenors for policy analysis\n",
        "forecast_horizons = [1, 5, 22]  # 1 day, 1 week, 1 month ahead\n",
        "\n",
        "print(f\"\\nüéØ Creating target variables:\")\n",
        "print(f\"  ‚Ä¢ Target tenors: {target_tenors}\")\n",
        "print(f\"  ‚Ä¢ Forecast horizons: {forecast_horizons} days\")\n",
        "\n",
        "# Create target variables\n",
        "target_columns = []\n",
        "Y_data = {'date': df_scaled['date'].copy()}\n",
        "\n",
        "for tenor in target_tenors:\n",
        "    for horizon in forecast_horizons:\n",
        "        target_col = f'{tenor}_target_{horizon}d'\n",
        "        \n",
        "        # Use continuously compounded yields for targets\n",
        "        if f'{tenor}_cc' in df_scaled.columns:\n",
        "            Y_data[target_col] = df_scaled[f'{tenor}_cc'].shift(-horizon)  # Negative shift for future values\n",
        "        else:\n",
        "            # Fallback to original yields if CC not available\n",
        "            Y_data[target_col] = df_scaled[tenor].shift(-horizon)\n",
        "            \n",
        "        target_columns.append(target_col)\n",
        "\n",
        "# Create target matrix Y\n",
        "Y = pd.DataFrame(Y_data)\n",
        "\n",
        "print(f\"  ‚Ä¢ Target matrix shape: {Y.shape}\")\n",
        "print(f\"  ‚Ä¢ Target variables created: {len(target_columns)}\")\n",
        "\n",
        "# Remove rows with NaN targets (due to forward-looking nature)\n",
        "initial_samples = len(X)\n",
        "valid_mask = Y[target_columns].notna().all(axis=1)\n",
        "\n",
        "X_clean = X[valid_mask].reset_index(drop=True)\n",
        "Y_clean = Y[valid_mask].reset_index(drop=True)\n",
        "\n",
        "final_samples = len(X_clean)\n",
        "print(f\"\\nüìä Final dataset dimensions:\")\n",
        "print(f\"  ‚Ä¢ Samples before target cleaning: {initial_samples}\")\n",
        "print(f\"  ‚Ä¢ Samples after target cleaning: {final_samples}\")\n",
        "print(f\"  ‚Ä¢ Samples removed: {initial_samples - final_samples}\")\n",
        "\n",
        "# TRAIN/TEST SPLIT FOR FINAL DATA\n",
        "# ================================\n",
        "\n",
        "# Recalculate train/test split for clean data\n",
        "split_date_clean = X_clean['date'].quantile(0.8)\n",
        "train_mask_clean = X_clean['date'] <= split_date_clean\n",
        "test_mask_clean = X_clean['date'] > split_date_clean\n",
        "\n",
        "print(f\"\\nüìÖ Final train/test split:\")\n",
        "print(f\"  ‚Ä¢ Train samples: {train_mask_clean.sum()}\")\n",
        "print(f\"  ‚Ä¢ Test samples: {test_mask_clean.sum()}\")\n",
        "print(f\"  ‚Ä¢ Train period: {X_clean.loc[train_mask_clean, 'date'].min()} to {X_clean.loc[train_mask_clean, 'date'].max()}\")\n",
        "print(f\"  ‚Ä¢ Test period: {X_clean.loc[test_mask_clean, 'date'].min()} to {X_clean.loc[test_mask_clean, 'date'].max()}\")\n",
        "\n",
        "# Separate features and targets for modeling\n",
        "X_features = X_clean.drop('date', axis=1)\n",
        "Y_targets = Y_clean.drop('date', axis=1)\n",
        "\n",
        "# Create train/test splits\n",
        "X_train = X_features[train_mask_clean]\n",
        "X_test = X_features[test_mask_clean]\n",
        "Y_train = Y_targets[train_mask_clean]\n",
        "Y_test = Y_targets[test_mask_clean]\n",
        "\n",
        "print(f\"\\nüéØ Final modeling datasets:\")\n",
        "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
        "print(f\"  ‚Ä¢ X_test shape: {X_test.shape}\")\n",
        "print(f\"  ‚Ä¢ Y_train shape: {Y_train.shape}\")\n",
        "print(f\"  ‚Ä¢ Y_test shape: {Y_test.shape}\")\n",
        "\n",
        "# FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================\n",
        "\n",
        "print(f\"\\nüîç Feature importance analysis...\")\n",
        "\n",
        "# Group features by category for analysis\n",
        "feature_analysis = {\n",
        "    'Yield Curve Features': [f for f in yield_curve_features if f in X_features.columns],\n",
        "    'Macro Features': [f for f in all_macro_features if f in X_features.columns],\n",
        "}\n",
        "\n",
        "print(f\"\\nFeature categories in final dataset:\")\n",
        "for category, features in feature_analysis.items():\n",
        "    print(f\"  ‚Ä¢ {category}: {len(features)} features\")\n",
        "\n",
        "# Calculate feature correlations with first target (2Y_target_1d)\n",
        "if len(Y_targets.columns) > 0:\n",
        "    target_sample = Y_targets.columns[0]\n",
        "    correlations = X_features.corrwith(Y_targets[target_sample])\n",
        "    \n",
        "    print(f\"\\nüìä Top 10 features correlated with {target_sample}:\")\n",
        "    top_correlations = correlations.abs().sort_values(ascending=False).head(10)\n",
        "    for feature, corr in top_correlations.items():\n",
        "        print(f\"  ‚Ä¢ {feature}: {corr:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Model-ready data construction completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 8. Data Export and Documentation\n",
        "\n",
        "Save all processed datasets to `/data/processed/` with comprehensive documentation for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîÑ Saving processed datasets...\")\n",
        "\n",
        "# Create timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 1. SAVE MAIN DATASETS\n",
        "# =====================\n",
        "\n",
        "# Complete feature-engineered dataset\n",
        "df_scaled.to_csv(f'../data/processed/complete_dataset_{timestamp}.csv', index=False)\n",
        "print(f\"‚úÖ Complete dataset saved: complete_dataset_{timestamp}.csv\")\n",
        "\n",
        "# Feature matrix X (with date)\n",
        "X_clean.to_csv(f'../data/processed/feature_matrix_{timestamp}.csv', index=False)\n",
        "print(f\"‚úÖ Feature matrix saved: feature_matrix_{timestamp}.csv\")\n",
        "\n",
        "# Target matrix Y (with date)\n",
        "Y_clean.to_csv(f'../data/processed/target_matrix_{timestamp}.csv', index=False)\n",
        "print(f\"‚úÖ Target matrix saved: target_matrix_{timestamp}.csv\")\n",
        "\n",
        "# 2. SAVE TRAIN/TEST SPLITS\n",
        "# =========================\n",
        "\n",
        "# Training sets\n",
        "X_train.to_csv(f'../data/processed/X_train_{timestamp}.csv', index=False)\n",
        "Y_train.to_csv(f'../data/processed/Y_train_{timestamp}.csv', index=False)\n",
        "\n",
        "# Test sets  \n",
        "X_test.to_csv(f'../data/processed/X_test_{timestamp}.csv', index=False)\n",
        "Y_test.to_csv(f'../data/processed/Y_test_{timestamp}.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Train/test splits saved with timestamp: {timestamp}\")\n",
        "\n",
        "# 3. SAVE FEATURE METADATA\n",
        "# ========================\n",
        "\n",
        "feature_metadata = {\n",
        "    'processing_date': timestamp,\n",
        "    'dataset_info': {\n",
        "        'total_samples': len(X_clean),\n",
        "        'total_features': len(X_features.columns),\n",
        "        'total_targets': len(Y_targets.columns),\n",
        "        'train_samples': len(X_train),\n",
        "        'test_samples': len(X_test),\n",
        "        'date_range': {\n",
        "            'start': str(X_clean['date'].min()),\n",
        "            'end': str(X_clean['date'].max())\n",
        "        }\n",
        "    },\n",
        "    'feature_groups': {\n",
        "        'yield_curve_features': {\n",
        "            'count': len([f for f in yield_curve_features if f in X_features.columns]),\n",
        "            'features': [f for f in yield_curve_features if f in X_features.columns]\n",
        "        },\n",
        "        'macro_features': {\n",
        "            'count': len([f for f in all_macro_features if f in X_features.columns]),\n",
        "            'features': [f for f in all_macro_features if f in X_features.columns]\n",
        "        }\n",
        "    },\n",
        "    'target_variables': {\n",
        "        'tenors': target_tenors,\n",
        "        'horizons': forecast_horizons,\n",
        "        'columns': target_columns\n",
        "    },\n",
        "    'transformations_applied': {\n",
        "        'yield_transformation': 'continuously_compounded',\n",
        "        'standardization': 'z_score_by_feature_group',\n",
        "        'lag_creation': 'macro_variables_lagged',\n",
        "        'pca_components': 5\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save feature metadata\n",
        "with open(f'../data/processed/feature_metadata_{timestamp}.json', 'w') as f:\n",
        "    json.dump(feature_metadata, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Feature metadata saved: feature_metadata_{timestamp}.json\")\n",
        "\n",
        "# 4. SAVE SCALERS FOR FUTURE USE\n",
        "# ==============================\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save all scalers\n",
        "with open(f'../data/processed/scalers_{timestamp}.pkl', 'wb') as f:\n",
        "    pickle.dump(scalers, f)\n",
        "\n",
        "# Save PCA transformer\n",
        "with open(f'../data/processed/pca_transformer_{timestamp}.pkl', 'wb') as f:\n",
        "    pickle.dump({'pca': pca, 'scaler': scaler_pca}, f)\n",
        "\n",
        "print(f\"‚úÖ Scalers and transformers saved\")\n",
        "\n",
        "# 5. CREATE DATA DICTIONARY\n",
        "# =========================\n",
        "\n",
        "print(f\"\\nüìù Creating comprehensive data dictionary...\")\n",
        "\n",
        "data_dictionary = {\n",
        "    'dataset_overview': {\n",
        "        'purpose': 'Yield curve forecasting and monetary policy scenario analysis',\n",
        "        'frequency': 'Daily business days',\n",
        "        'date_range': f\"{X_clean['date'].min()} to {X_clean['date'].max()}\",\n",
        "        'total_observations': len(X_clean)\n",
        "    },\n",
        "    'feature_descriptions': {},\n",
        "    'target_descriptions': {},\n",
        "    'data_sources': {\n",
        "        'yield_curves': 'US Treasury (FRED API simulation)',\n",
        "        'macro_indicators': 'Federal Reserve Economic Data (FRED API simulation)',\n",
        "        'transformations': 'Custom feature engineering pipeline'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Add feature descriptions\n",
        "feature_descriptions = {\n",
        "    # Yield curve features\n",
        "    'yield_slope_10y2y': '10-Year minus 2-Year Treasury yield spread',\n",
        "    'yield_curvature': '(2Y + 30Y) - 2*(10Y) - captures mid-curve behavior',\n",
        "    'pca_factor_1': 'First principal component (typically level factor)',\n",
        "    'pca_factor_2': 'Second principal component (typically slope factor)', \n",
        "    'pca_factor_3': 'Third principal component (typically curvature factor)',\n",
        "    'yield_level': 'Average of all yield tenors',\n",
        "    'yield_range': 'Range (max - min) of yield curve',\n",
        "    \n",
        "    # Macro features examples\n",
        "    'fed_funds_rate_lag1': 'Federal Funds Rate lagged 1 day',\n",
        "    'vix_log_lag1': 'Log-transformed VIX lagged 1 day',\n",
        "    'inflation_term_spread': '10Y - 5Y breakeven inflation spread',\n",
        "    'economic_momentum': 'Composite economic activity indicator'\n",
        "}\n",
        "\n",
        "data_dictionary['feature_descriptions'] = feature_descriptions\n",
        "\n",
        "# Add target descriptions\n",
        "target_descriptions = {}\n",
        "for tenor in target_tenors:\n",
        "    for horizon in forecast_horizons:\n",
        "        target_descriptions[f'{tenor}_target_{horizon}d'] = f'{tenor} Treasury yield {horizon} days ahead'\n",
        "\n",
        "data_dictionary['target_descriptions'] = target_descriptions\n",
        "\n",
        "# Save data dictionary\n",
        "with open(f'../data/processed/data_dictionary_{timestamp}.json', 'w') as f:\n",
        "    json.dump(data_dictionary, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Data dictionary saved: data_dictionary_{timestamp}.json\")\n",
        "\n",
        "# 6. FINAL SUMMARY REPORT\n",
        "# =======================\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(f\"üéâ FEATURE ENGINEERING PHASE 2 COMPLETED SUCCESSFULLY\")\n",
        "print(f\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä FINAL DATASET SUMMARY:\")\n",
        "print(f\"  ‚Ä¢ Total observations: {len(X_clean):,}\")\n",
        "print(f\"  ‚Ä¢ Total features: {len(X_features.columns):,}\")\n",
        "print(f\"  ‚Ä¢ Total targets: {len(Y_targets.columns):,}\")\n",
        "print(f\"  ‚Ä¢ Training samples: {len(X_train):,}\")\n",
        "print(f\"  ‚Ä¢ Test samples: {len(X_test):,}\")\n",
        "print(f\"  ‚Ä¢ Date range: {X_clean['date'].min()} to {X_clean['date'].max()}\")\n",
        "\n",
        "print(f\"\\nüéØ TARGET VARIABLES:\")\n",
        "for i, target in enumerate(target_columns):\n",
        "    print(f\"  {i+1:2d}. {target}\")\n",
        "\n",
        "print(f\"\\nüíæ FILES SAVED TO /data/processed/:\")\n",
        "print(f\"  ‚Ä¢ complete_dataset_{timestamp}.csv\")\n",
        "print(f\"  ‚Ä¢ feature_matrix_{timestamp}.csv\") \n",
        "print(f\"  ‚Ä¢ target_matrix_{timestamp}.csv\")\n",
        "print(f\"  ‚Ä¢ X_train_{timestamp}.csv / Y_train_{timestamp}.csv\")\n",
        "print(f\"  ‚Ä¢ X_test_{timestamp}.csv / Y_test_{timestamp}.csv\")\n",
        "print(f\"  ‚Ä¢ feature_metadata_{timestamp}.json\")\n",
        "print(f\"  ‚Ä¢ data_dictionary_{timestamp}.json\")\n",
        "print(f\"  ‚Ä¢ scalers_{timestamp}.pkl\")\n",
        "print(f\"  ‚Ä¢ pca_transformer_{timestamp}.pkl\")\n",
        "print(f\"  ‚Ä¢ scaling_metadata.json\")\n",
        "\n",
        "print(f\"\\nüîß TRANSFORMATIONS APPLIED:\")\n",
        "print(f\"  ‚úÖ Data alignment and gap filling\")\n",
        "print(f\"  ‚úÖ Yield transformation to continuously compounded rates\")\n",
        "print(f\"  ‚úÖ Yield curve feature engineering (slope, curvature, PCA)\")\n",
        "print(f\"  ‚úÖ Macroeconomic feature engineering with proper lags\")\n",
        "print(f\"  ‚úÖ Z-score standardization by feature groups\")\n",
        "print(f\"  ‚úÖ Model-ready train/test splits\")\n",
        "\n",
        "print(f\"\\nüöÄ READY FOR PHASE 3: MODEL DEVELOPMENT\")\n",
        "print(f\"   The processed datasets are now ready for:\")\n",
        "print(f\"   ‚Ä¢ Baseline model training\")\n",
        "print(f\"   ‚Ä¢ Advanced ML model development\")\n",
        "print(f\"   ‚Ä¢ Model comparison and evaluation\")\n",
        "print(f\"   ‚Ä¢ Explainability analysis\")\n",
        "print(f\"   ‚Ä¢ Policy scenario simulation\")\n",
        "\n",
        "print(f\"\\nüìù NEXT STEPS:\")\n",
        "print(f\"   1. Load processed data: pd.read_csv('data/processed/feature_matrix_{timestamp}.csv')\")\n",
        "print(f\"   2. Train baseline models (Random Walk, VAR, Linear Regression)\")\n",
        "print(f\"   3. Develop ML models (Random Forest, XGBoost, LSTM)\")\n",
        "print(f\"   4. Evaluate and compare model performance\")\n",
        "print(f\"   5. Conduct explainability analysis\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 02. Feature Engineering for Yield Curve Modeling\n",
        "\n",
        "## Purpose\n",
        "This notebook transforms raw yield curve data and macroeconomic indicators into engineered features suitable for machine learning models. It creates both traditional financial features and advanced technical indicators.\n",
        "\n",
        "## Objectives\n",
        "1. **Create yield curve factors** - level, slope, curvature, twist, butterfly\n",
        "2. **Generate technical indicators** - moving averages, RSI, Bollinger bands, MACD\n",
        "3. **Engineer macroeconomic features** - growth rates, lags, volatility measures\n",
        "4. **Create lag features** for time series modeling\n",
        "5. **Apply feature scaling and normalization**\n",
        "6. **Perform feature selection** to identify most informative variables\n",
        "7. **Generate interaction features** between yield curve and macro variables\n",
        "\n",
        "## Expected Outputs\n",
        "- Engineered feature dataset ready for modeling\n",
        "- Feature importance rankings\n",
        "- Feature correlation analysis\n",
        "- Scaled and normalized features\n",
        "- Feature metadata and documentation\n",
        "\n",
        "## Dependencies\n",
        "- Cleaned yield curve data from notebook 01\n",
        "- Processed macroeconomic data\n",
        "- Feature engineering configuration parameters\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
